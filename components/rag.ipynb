{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This our RAG which is organized in one function\n",
    "def query_answering_system(query, document_dataset, embedding_model_name='BAAI/bge-small-en', model_name=\"Qwen/Qwen2.5-0.5B\", k=3, embedding_cache_path='../data//document_embeddings.npy'):\n",
    "    \"\"\"\n",
    "    Given a query and a document dataset, this function retrieves relevant documents and generates an answer.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user query.\n",
    "    - document_dataset (str): Path to the CSV file containing documents with 'title' in column index 1 and 'text' in column index 2.\n",
    "    - embedding_model_name (str): Name of the sentence embedding model.\n",
    "    - model_name (str): Name of the language model.\n",
    "    - k (int): Number of top similar documents to retrieve.\n",
    "    - embedding_cache_path (str): Path to store/load document embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - answer (str): Generated response based on retrieved documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load pre-trained models\n",
    "    embedding_model = SentenceTransformer(embedding_model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    generator_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(document_dataset, encoding='utf-8')\n",
    "    documents = [f\"title: {d[1]}.  text: {d[2]}\" for d in df.values.tolist()]\n",
    "\n",
    "    # Function to generate embeddings\n",
    "    def generate_embeddings(texts):\n",
    "        return embedding_model.encode(texts, show_progress_bar=True, batch_size=160, device=device)\n",
    "\n",
    "    # Generate/load document embeddings\n",
    "    if not os.path.exists(embedding_cache_path):\n",
    "        documents_embedding = generate_embeddings(documents)\n",
    "        np.save(embedding_cache_path, documents_embedding)\n",
    "    else:\n",
    "        documents_embedding = np.load(embedding_cache_path)\n",
    "\n",
    "    # Retrieve top-k similar documents\n",
    "    query_embedding = generate_embeddings([query])\n",
    "    similarities = cosine_similarity(query_embedding, documents_embedding)\n",
    "    most_similar_indices = similarities.argsort()[0][-k:][::-1]\n",
    "    retrieved_docs = [documents[i] for i in most_similar_indices]\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = \"Given the following documents:\\n\"\n",
    "    prompt += \"\\n\".join(f\"{i+1}. {doc}\" for i, doc in enumerate(retrieved_docs))\n",
    "    prompt += f\"\\n\\nUser query: {query}\\n\\n\"\n",
    "    prompt += \"Based on the above documents, provide a concise, clear, and logically structured answer to the user's query.\\n\"\n",
    "    prompt += \"Also please give me the basis for your answer.\"\n",
    "\n",
    "    # Generate response\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(generator_model.device)\n",
    "\n",
    "    generated_ids = generator_model.generate(**model_inputs, max_new_tokens=512)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    answer = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of RAG:\n",
    "query = \"What option do civil servants in Malaysia have for their working hours during Ramadan, according to Communications Minister Fahmi Fadzil?\"\n",
    "document_dataset = \"../data/1K_news.csv\"\n",
    "result = query_answering_system(query, document_dataset)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
