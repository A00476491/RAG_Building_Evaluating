{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This our RAG which is organized in one function\n",
    "def query_answering_system(query, document_dataset, embedding_model_name='BAAI/bge-small-en', model_name=\"Qwen/Qwen2.5-0.5B\", k=3, embedding_cache_path='./document_embeddings.npy'):\n",
    "    \"\"\"\n",
    "    Given a query and a document dataset, this function retrieves relevant documents and generates an answer.\n",
    "\n",
    "    Parameters:\n",
    "    - query (str): The user query.\n",
    "    - document_dataset (str): Path to the CSV file containing documents with 'title' in column index 1 and 'text' in column index 2.\n",
    "    - embedding_model_name (str): Name of the sentence embedding model.\n",
    "    - model_name (str): Name of the language model.\n",
    "    - k (int): Number of top similar documents to retrieve.\n",
    "    - embedding_cache_path (str): Path to store/load document embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - answer (str): Generated response based on retrieved documents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Load pre-trained models\n",
    "    embedding_model = SentenceTransformer(embedding_model_name).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    generator_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)\n",
    "\n",
    "    # Load dataset\n",
    "    df = pd.read_csv(document_dataset, encoding='utf-8')\n",
    "    documents = [f\"title: {d[1]}.  text: {d[2]}\" for d in df.values.tolist()]\n",
    "\n",
    "    # Function to generate embeddings\n",
    "    def generate_embeddings(texts):\n",
    "        return embedding_model.encode(texts, show_progress_bar=True, batch_size=160, device=device)\n",
    "\n",
    "    # Generate/load document embeddings\n",
    "    if not os.path.exists(embedding_cache_path):\n",
    "        documents_embedding = generate_embeddings(documents)\n",
    "        np.save(embedding_cache_path, documents_embedding)\n",
    "    else:\n",
    "        documents_embedding = np.load(embedding_cache_path)\n",
    "\n",
    "    # Retrieve top-k similar documents\n",
    "    query_embedding = generate_embeddings([query])\n",
    "    similarities = cosine_similarity(query_embedding, documents_embedding)\n",
    "    most_similar_indices = similarities.argsort()[0][-k:][::-1]\n",
    "    retrieved_docs = [documents[i] for i in most_similar_indices]\n",
    "\n",
    "    # Construct the prompt\n",
    "    prompt = \"Given the following documents:\\n\"\n",
    "    prompt += \"\\n\".join(f\"{i+1}. {doc}\" for i, doc in enumerate(retrieved_docs))\n",
    "    prompt += f\"\\n\\nUser query: {query}\\n\\n\"\n",
    "    prompt += \"Based on the above documents, provide a concise, clear, and logically structured answer to the user's query.\\n\"\n",
    "    prompt += \"Also please give me the basis for your answer.\"\n",
    "\n",
    "    # Generate response\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(generator_model.device)\n",
    "\n",
    "    generated_ids = generator_model.generate(**model_inputs, max_new_tokens=512)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]\n",
    "    answer = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of RAG:\n",
    "query = \"What option do civil servants in Malaysia have for their working hours during Ramadan, according to Communications Minister Fahmi Fadzil?\"\n",
    "document_dataset = \"./data/1K_news.csv\"\n",
    "result = query_answering_system(query, document_dataset)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()\n",
    "\n",
    "df = pd.read_csv('./data/1K_news.csv', encoding='utf-8')\n",
    "client.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Ensure df exists and contains the necessary columns\n",
    "df = df.head(50)  # Select the first 50 rows\n",
    "\n",
    "def generate_question(text):\n",
    "    prompt = (\n",
    "        \"Generate a simple, easy-to-understand and not too long question based on the following news content.\"\n",
    "        \"Also, the question must be specific and clear. For example, instead of using 'servants,' it should specify which region or countryâ€™s servants are being referred to.\"\n",
    "        \"Additionally, the question should not be too difficult, and the answer must be explicitly contained within the following News content.\\n\\n\"\n",
    "        f\"News content:\\n{text}\\n\\n\"\n",
    "        \"Question:\"\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Apply the function to generate questions\n",
    "df[\"generated_question\"] = df[\"text\"].apply(generate_question)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv(\"./data/50_news_questions.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Question generation completed. Results saved to news_questions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/50_news_questions.csv', encoding='utf-8')\n",
    "document_dataset = \"./data/1K_news.csv\"\n",
    "for i in range(len(df)):\n",
    "    query = df.loc[i, 'generated_question']\n",
    "    generated_response = query_answering_system(query, document_dataset)\n",
    "    query = df.loc[i, 'generated_response'] = generated_response\n",
    "\n",
    "    print(i)\n",
    "\n",
    "df.to_csv(\"./data/50_news_QA.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
