{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from components import rag\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of RAG:\n",
    "query = \"What is the main goal of the project launched in Cambodia on February 27 to improve plastic waste management?\"\n",
    "document_dataset = \"./data/1K_news.csv\"\n",
    "output = rag.query_answering_system(query, document_dataset)\n",
    "print('Answer: {}'.format(output['answer']))\n",
    "print('Reference1: {}'.format(output['title'][0]))\n",
    "print('Reference1: {}'.format(output['title'][1]))\n",
    "print('Reference1: {}'.format(output['title'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate questions by Chatgpt\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "df_1K = pd.read_csv('./data/1K_news.csv', encoding='utf-8')\n",
    "\n",
    "\n",
    "# Ensure df exists and contains the necessary columns\n",
    "df = df_1K.head(100)  # Select the first 50 rows\n",
    "\n",
    "def generate_question(text):\n",
    "    prompt = (\n",
    "        \"Generate a simple, easy-to-understand and not too long question based on the following news content.\"\n",
    "        \"Also, the question must be specific and clear. For example, instead of using 'servants,' it should specify which region or countryâ€™s servants are being referred to.\"\n",
    "        \"Additionally, the question should not be too difficult, and the answer must be explicitly contained within the following News content.\\n\\n\"\n",
    "        f\"News content:\\n{text}\\n\\n\"\n",
    "        \"Question:\"\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "# Apply the function to generate questions\n",
    "df[\"generated_question\"] = df[\"text\"].apply(generate_question)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv(\"./data/Q6/100_news_questions.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Question generation completed. Results saved to news_questions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers by Rag\n",
    "df = pd.read_csv('./data/Q6/100_news_questions.csv', encoding='utf-8')\n",
    "# df = df.head(5)\n",
    "document_dataset = \"./data/1K_news.csv\"\n",
    "for i in range(len(df)):\n",
    "    query = df.loc[i, 'generated_question']\n",
    "    output = rag.query_answering_system(query, document_dataset)\n",
    "    query = df.loc[i, 'generated_response'] = output['answer']\n",
    "    query = df.loc[i, 'retrieved_docs_id'] = str(output['retrieved_docs_id'])[1:-1]\n",
    "\n",
    "    print(i)\n",
    "\n",
    "df.to_csv(\"./data/Q6/100_news_QA.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision@K\n",
    "df = pd.read_csv('./data/Q6/100_news_QA.csv', encoding='utf-8')\n",
    "TP_3 = 0\n",
    "TP_2 = 0\n",
    "TP_1 = 0\n",
    "for i in range(len(df)):\n",
    "\n",
    "    content_id = str(df['content_id'][i])\n",
    "    retrieved_docs_id = df['retrieved_docs_id'][i].split(', ')\n",
    "    # print(content_id)\n",
    "    # print(retrieved_docs_id)\n",
    "\n",
    "    if content_id in retrieved_docs_id:\n",
    "        TP_3 += 1\n",
    "        TP_2 += 1\n",
    "        TP_1 += 1\n",
    "    elif content_id in retrieved_docs_id[:-1]:\n",
    "        TP_2 += 1\n",
    "        TP_1 += 1\n",
    "    elif content_id in retrieved_docs_id[:-2]:\n",
    "        TP_1 += 1\n",
    "\n",
    "Precision_top3 = TP_3 / len(df)\n",
    "Precision_top2 = TP_2 / len(df)\n",
    "Precision_top1 = TP_1 / len(df)\n",
    "\n",
    "print(f\"Precision@3: {Precision_top3:.4f}\")\n",
    "print(f\"Precision@2: {Precision_top2:.4f}\")\n",
    "print(f\"Precision@1: {Precision_top1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Chat GPT evaluate if Rag has model hallucinated\n",
    "client = OpenAI()\n",
    "client.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "def evaluate_rag_hallucinated(text, question, answer):\n",
    "    prompt = (\n",
    "        \"Evaluate whether the Knowledge-based AI assistant's answer is factually accurate based only on the given documents.\\n\"\n",
    "        \"Your response should be in two rows:\\n\"\n",
    "        \"1. First row: True or False (True if the answer is fully supported by the documents, False if it contains hallucinated or unverifiable information).\\n\"\n",
    "        \"2. Second row: A short explanation for your decision.\\n\\n\"\n",
    "        \"Documents:\\n\"\n",
    "        f\"{text}\\n\\n\"\n",
    "        \"Question:\\n\"\n",
    "        f\"{question}\\n\\n\"\n",
    "        \"Answer from the AI assistant:\\n\"\n",
    "        f\"{answer}\\n\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "    # print(prompt)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion.choices[0].message.content.strip()\n",
    "\n",
    "df = pd.read_csv('./data/Q6/100_news_QA.csv', encoding='utf-8')\n",
    "df_1K = pd.read_csv('./data/1K_news.csv', encoding='utf-8')\n",
    "\n",
    "df[\"hallucinated\"] = None\n",
    "df[\"reason\"] = None\n",
    "\n",
    "for i in range(len(df)):\n",
    "\n",
    "    doc_ids = list(map(int, df[\"retrieved_docs_id\"][i].split(\", \")))\n",
    "    text_docs = df_1K[df_1K['content_id'].isin(doc_ids)]['text']\n",
    "    text = \"\\n\\n\".join([f\"document{j+1}: {text_docs.iloc[j]}\" for j in range(min(3, len(text_docs)))])\n",
    "        \n",
    "    isHallucinated, reason = evaluate_rag_hallucinated(text, df.loc[i, \"generated_question\"], df.loc[i, \"generated_response\"]).splitlines()\n",
    "    df.loc[i, 'hallucinated'] = isHallucinated\n",
    "    df.loc[i, 'reason'] = reason\n",
    "\n",
    "print(df['hallucinated'])\n",
    "df.to_csv(\"./data/Q6/100_news_QA.csv\", index=False, encoding='utf-8')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df['hallucinated'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
